#!/usr/bin/env python3
"""
QUALITY VALIDATION SUITE v1.0

PURPOSE: Validate enhanced insights meet A+ quality standards
CRITERIA: Based on Agent 2's violation report and A+ requirements
TARGETS: Content processed by all enhancement scripts

Validates: Clarity, Length, Human Anchoring, Persona Voice, No Template Artifacts
"""

import json
import logging
import re
import statistics
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class QualityValidationSuite:
    def __init__(self):
        """Initialize the quality validation system."""

        # A+ QUALITY CRITERIA
        self.quality_criteria = {
            "clarity_readability": {
                "weight": 0.25,
                "checks": [
                    "no_template_artifacts",
                    "coherent_sentences",
                    "proper_grammar",
                    "clear_meaning",
                ],
            },
            "length_optimization": {
                "weight": 0.20,
                "checks": ["word_count_range", "concise_delivery", "no_verbosity"],
            },
            "human_anchoring": {
                "weight": 0.25,
                "checks": [
                    "first_person_percentage",
                    "action_verbs_present",
                    "concrete_guidance",
                    "no_prayer_style",
                ],
            },
            "persona_authenticity": {
                "weight": 0.20,
                "checks": ["consistent_voice", "archetypal_depth", "organic_integration"],
            },
            "spiritual_value": {
                "weight": 0.10,
                "checks": ["meaningful_wisdom", "practical_application", "inspirational_quality"],
            },
        }

        # VIOLATION PATTERNS (from Agent 2's audit)
        self.violation_patterns = {
            "template_artifacts": [
                r"Your primal initiation harmonizes",
                r"- sacred power flows",
                r"ignites before thought itself",
                r"cosmic communion births through",
                r"seasonal communion births through",
                r"The warrior archetype activates",
                r"The pioneer archetype illuminates",
            ],
            "buzzwords": [
                r"\bdivine\b",
                r"\bsacred\b",
                r"\bmystical\b",
                r"\bcosmic\b",
                r"\buniversal\b",
                r"\bethereal\b",
                r"\bcelestial\b",
            ],
            "template_language": [
                r"aligns with",
                r"resonates with",
                r"vibrates at",
                r"harmonizes",
                r"awakens",
                r"illuminates",
            ],
            "prayer_style": [r"\bMay you\b", r"\bMay this\b", r"\bMay your\b"],
            "vague_concepts": [r"energy flows", r"light enters", r"wisdom flows"],
        }

        # WORD COUNT TARGETS
        self.length_targets = {"min_words": 15, "max_words": 25, "absolute_max": 30}

        # FIRST-PERSON TARGET
        self.first_person_target = 25.0  # 25% minimum

        # STATISTICS
        self.validation_stats = {
            "files_validated": 0,
            "insights_tested": 0,
            "a_plus_insights": 0,
            "violations_found": 0,
            "avg_quality_score": 0.0,
            "criteria_failures": {},
            "errors_encountered": 0,
        }

        # Initialize criteria failure tracking
        for criterion in self.quality_criteria.keys():
            self.validation_stats["criteria_failures"][criterion] = 0

    def count_words(self, text: str) -> int:
        """Count words in text, excluding punctuation."""
        words = re.findall(r"\b\w+\b", text)
        return len(words)

    def calculate_first_person_percentage(self, text: str) -> float:
        """Calculate percentage of first-person language."""
        words = text.split()
        if not words:
            return 0.0

        first_person_indicators = ["I", "me", "my", "mine", "myself"]
        first_person_count = sum(
            1 for word in words if re.sub(r"[^\w]", "", word) in first_person_indicators
        )

        return (first_person_count / len(words)) * 100

    def check_clarity_readability(self, text: str) -> Tuple[float, List[str]]:
        """
        Check clarity and readability criteria.

        Returns:
            tuple: (score_0_to_1, list_of_issues)
        """
        issues = []
        score_components = []

        # Check 1: No template artifacts
        artifact_violations = 0
        for pattern in self.violation_patterns["template_artifacts"]:
            if re.search(pattern, text, re.IGNORECASE):
                artifact_violations += 1
                issues.append(f"Template artifact found: {pattern}")

        artifact_score = max(0, 1 - (artifact_violations * 0.5))
        score_components.append(artifact_score)

        # Check 2: Coherent sentences (no fragmented text)
        fragmentation_score = 1.0
        if re.search(r"\w+\s+You are\s+\w+", text):  # Fragmented merging
            fragmentation_score = 0.3
            issues.append("Fragmented sentence structure detected")
        elif re.search(r"\w+\s+-\s+\w+\s+\w+", text):  # Dash fragments
            fragmentation_score = 0.5
            issues.append("Dash fragment detected")

        score_components.append(fragmentation_score)

        # Check 3: Proper grammar (basic checks)
        grammar_score = 1.0
        if re.search(r"\.\s*[a-z]", text):  # Lowercase after period
            grammar_score = 0.7
            issues.append("Capitalization error after period")
        if re.search(r"[,\.]\s*[,\.]+", text):  # Double punctuation
            grammar_score = 0.8
            issues.append("Double punctuation found")

        score_components.append(grammar_score)

        # Check 4: Clear meaning (not too many buzzwords)
        buzzword_count = 0
        for pattern in self.violation_patterns["buzzwords"]:
            buzzword_count += len(re.findall(pattern, text, re.IGNORECASE))

        meaning_score = max(0.2, 1 - (buzzword_count * 0.2))
        score_components.append(meaning_score)

        overall_score = statistics.mean(score_components)
        return overall_score, issues

    def check_length_optimization(self, text: str) -> Tuple[float, List[str]]:
        """
        Check length optimization criteria.

        Returns:
            tuple: (score_0_to_1, list_of_issues)
        """
        issues = []
        word_count = self.count_words(text)

        # Optimal range scoring
        if self.length_targets["min_words"] <= word_count <= self.length_targets["max_words"]:
            length_score = 1.0
        elif word_count < self.length_targets["min_words"]:
            length_score = 0.7
            issues.append(
                f"Too short: {word_count} words (target: {self.length_targets['min_words']}-{self.length_targets['max_words']})"
            )
        elif word_count <= self.length_targets["absolute_max"]:
            length_score = 0.8
            issues.append(
                f"Slightly long: {word_count} words (target: {self.length_targets['min_words']}-{self.length_targets['max_words']})"
            )
        else:
            length_score = 0.4
            issues.append(
                f"Too long: {word_count} words (max: {self.length_targets['absolute_max']})"
            )

        # Check for verbosity indicators
        verbose_patterns = [
            r"in order to",
            r"for the purpose of",
            r"with the intention of",
            r"at this point in time",
            r"due to the fact that",
        ]

        verbosity_penalty = 0
        for pattern in verbose_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                verbosity_penalty += 0.1
                issues.append(f"Verbose phrase found: {pattern}")

        final_score = max(0, length_score - verbosity_penalty)
        return final_score, issues

    def check_human_anchoring(self, text: str) -> Tuple[float, List[str]]:
        """
        Check human anchoring criteria.

        Returns:
            tuple: (score_0_to_1, list_of_issues)
        """
        issues = []
        score_components = []

        # Check 1: First-person percentage
        first_person_pct = self.calculate_first_person_percentage(text)
        if first_person_pct >= self.first_person_target:
            fp_score = 1.0
        elif first_person_pct >= 15:
            fp_score = 0.7
            issues.append(
                f"Low first-person: {first_person_pct:.1f}% (target: {self.first_person_target}%)"
            )
        else:
            fp_score = 0.3
            issues.append(
                f"Very low first-person: {first_person_pct:.1f}% (target: {self.first_person_target}%)"
            )

        score_components.append(fp_score)

        # Check 2: Action verbs present
        action_verbs = [
            "notice",
            "choose",
            "try",
            "write",
            "schedule",
            "practice",
            "explore",
            "observe",
            "reflect",
            "consider",
            "create",
            "build",
            "develop",
        ]

        action_count = 0
        for verb in action_verbs:
            if re.search(r"\b" + verb + r"\b", text, re.IGNORECASE):
                action_count += 1

        action_score = min(1.0, action_count * 0.5)
        if action_score < 0.5:
            issues.append(f"Few action verbs found: {action_count}")

        score_components.append(action_score)

        # Check 3: No prayer-style language
        prayer_violations = 0
        for pattern in self.violation_patterns["prayer_style"]:
            if re.search(pattern, text, re.IGNORECASE):
                prayer_violations += 1
                issues.append(f"Prayer-style language: {pattern}")

        prayer_score = max(0, 1 - (prayer_violations * 0.3))
        score_components.append(prayer_score)

        overall_score = statistics.mean(score_components)
        return overall_score, issues

    def check_persona_authenticity(
        self, text: str, persona: Optional[str] = None
    ) -> Tuple[float, List[str]]:
        """
        Check persona authenticity criteria.

        Returns:
            tuple: (score_0_to_1, list_of_issues)
        """
        issues = []

        # Check for template merging patterns (indicates inauthentic generation)
        template_merging_patterns = [
            r"\w+\s+You are the\s+\w+",  # Fragmented merging
            r"harmonizes\s+\w+\s+is asking",  # Template collision
            r"archetype\s+\w+\s+cosmic",  # Archetypal template bleeding
        ]

        authenticity_score = 1.0
        for pattern in template_merging_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                authenticity_score -= 0.3
                issues.append(f"Template merging detected: {pattern}")

        # Check for mechanical repetition
        words = text.lower().split()
        word_freq = {}
        for word in words:
            clean_word = re.sub(r"[^\w]", "", word)
            if len(clean_word) > 3:  # Only check meaningful words
                word_freq[clean_word] = word_freq.get(clean_word, 0) + 1

        repetition_penalty = 0
        for word, count in word_freq.items():
            if count > 2:  # Word used more than twice
                repetition_penalty += 0.1
                issues.append(f"Repetitive word: '{word}' used {count} times")

        final_score = max(0.2, authenticity_score - repetition_penalty)
        return final_score, issues

    def check_spiritual_value(self, text: str) -> Tuple[float, List[str]]:
        """
        Check spiritual value criteria.

        Returns:
            tuple: (score_0_to_1, list_of_issues)
        """
        issues = []

        # Basic spiritual value assessment
        # Check for meaningful wisdom indicators
        wisdom_indicators = [
            "understand",
            "grow",
            "learn",
            "discover",
            "realize",
            "awareness",
            "insight",
            "wisdom",
            "truth",
            "purpose",
            "meaning",
            "guidance",
        ]

        wisdom_count = 0
        for indicator in wisdom_indicators:
            if re.search(r"\b" + indicator + r"\b", text, re.IGNORECASE):
                wisdom_count += 1

        wisdom_score = min(1.0, wisdom_count * 0.3)

        # Check for practical application
        practical_indicators = [
            "practice",
            "apply",
            "use",
            "implement",
            "start",
            "begin",
            "try",
            "create",
            "build",
            "develop",
            "choose",
            "decide",
        ]

        practical_count = 0
        for indicator in practical_indicators:
            if re.search(r"\b" + indicator + r"\b", text, re.IGNORECASE):
                practical_count += 1

        practical_score = min(1.0, practical_count * 0.4)

        overall_score = (wisdom_score + practical_score) / 2

        if overall_score < 0.5:
            issues.append("Low spiritual value - lacks wisdom or practical guidance")

        return overall_score, issues

    def validate_insight(self, text: str, persona: Optional[str] = None) -> Dict[str, Any]:
        """
        Perform comprehensive quality validation on a single insight.

        Returns:
            dict: Detailed validation results
        """
        results = {
            "text": text,
            "persona": persona,
            "word_count": self.count_words(text),
            "first_person_percentage": self.calculate_first_person_percentage(text),
            "scores": {},
            "issues": {},
            "overall_score": 0.0,
            "grade": "F",
            "a_plus_quality": False,
        }

        total_weighted_score = 0.0

        # Run all quality checks
        for criterion, config in self.quality_criteria.items():
            if criterion == "clarity_readability":
                score, issues = self.check_clarity_readability(text)
            elif criterion == "length_optimization":
                score, issues = self.check_length_optimization(text)
            elif criterion == "human_anchoring":
                score, issues = self.check_human_anchoring(text)
            elif criterion == "persona_authenticity":
                score, issues = self.check_persona_authenticity(text, persona)
            elif criterion == "spiritual_value":
                score, issues = self.check_spiritual_value(text)
            else:
                score, issues = 0.0, []

            results["scores"][criterion] = score
            results["issues"][criterion] = issues

            weighted_score = score * config["weight"]
            total_weighted_score += weighted_score

            # Track failures
            if score < 0.7:  # Below B- grade
                self.validation_stats["criteria_failures"][criterion] += 1

        results["overall_score"] = total_weighted_score

        # Assign letter grades
        if total_weighted_score >= 0.93:
            results["grade"] = "A+"
            results["a_plus_quality"] = True
            self.validation_stats["a_plus_insights"] += 1
        elif total_weighted_score >= 0.90:
            results["grade"] = "A"
        elif total_weighted_score >= 0.87:
            results["grade"] = "A-"
        elif total_weighted_score >= 0.83:
            results["grade"] = "B+"
        elif total_weighted_score >= 0.80:
            results["grade"] = "B"
        elif total_weighted_score >= 0.77:
            results["grade"] = "B-"
        elif total_weighted_score >= 0.73:
            results["grade"] = "C+"
        elif total_weighted_score >= 0.70:
            results["grade"] = "C"
        else:
            results["grade"] = "F"

        # Count violations
        total_issues = sum(len(issue_list) for issue_list in results["issues"].values())
        if total_issues > 0:
            self.validation_stats["violations_found"] += total_issues

        return results

    def validate_file(self, file_path: Path) -> Dict[str, Any]:
        """
        Validate all insights in a JSON file.

        Returns:
            dict: File validation summary
        """
        try:
            logger.info(f"Validating quality in: {file_path}")

            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            file_results = {
                "file_path": str(file_path),
                "insights_validated": 0,
                "a_plus_insights": 0,
                "avg_score": 0.0,
                "grade_distribution": {},
                "detailed_results": [],
            }

            scores = []
            persona = None

            # Extract persona if available
            if isinstance(data, dict) and "primary_persona" in data:
                persona = data["primary_persona"]

            # Process insights
            insights_processed = 0

            if isinstance(data, dict):
                # Handle different JSON structures
                for key, value in data.items():
                    # Handle numbered keys (e.g., "1", "2", "3" etc.)
                    if isinstance(value, dict) and "insight" in value:
                        insight_list = value["insight"]
                        if isinstance(insight_list, list):
                            for insight_item in insight_list:
                                if isinstance(insight_item, str):
                                    result = self.validate_insight(insight_item, persona)
                                    scores.append(result["overall_score"])
                                    file_results["detailed_results"].append(result)
                                    insights_processed += 1

                                    # Track grade distribution
                                    grade = result["grade"]
                                    file_results["grade_distribution"][grade] = (
                                        file_results["grade_distribution"].get(grade, 0) + 1
                                    )

                                    if result["a_plus_quality"]:
                                        file_results["a_plus_insights"] += 1
                    # Handle direct insight arrays
                    elif key in ["insight", "insights"] and isinstance(value, list):
                        for insight_item in value:
                            if isinstance(insight_item, str):
                                result = self.validate_insight(insight_item, persona)
                            elif isinstance(insight_item, dict):
                                # Extract text from dict
                                text_fields = ["insight", "text", "message", "content", "wisdom"]
                                insight_text = None
                                for field in text_fields:
                                    if field in insight_item:
                                        insight_text = insight_item[field]
                                        break

                                if insight_text:
                                    item_persona = insight_item.get("persona", persona)
                                    result = self.validate_insight(insight_text, item_persona)
                                else:
                                    continue
                            else:
                                continue

                            scores.append(result["overall_score"])
                            file_results["detailed_results"].append(result)
                            insights_processed += 1

                            # Track grade distribution
                            grade = result["grade"]
                            file_results["grade_distribution"][grade] = (
                                file_results["grade_distribution"].get(grade, 0) + 1
                            )

                            if result["a_plus_quality"]:
                                file_results["a_plus_insights"] += 1

            elif isinstance(data, list):
                # Direct list of insights
                for insight_item in data:
                    if isinstance(insight_item, str):
                        result = self.validate_insight(insight_item, persona)
                        scores.append(result["overall_score"])
                        file_results["detailed_results"].append(result)
                        insights_processed += 1

            # Calculate file statistics
            file_results["insights_validated"] = insights_processed
            if scores:
                file_results["avg_score"] = statistics.mean(scores)

            self.validation_stats["files_validated"] += 1
            self.validation_stats["insights_tested"] += insights_processed

            logger.info(f"‚úÖ Validated {insights_processed} insights in {file_path.name}")
            logger.info(f"   Average score: {file_results['avg_score']:.3f}")
            logger.info(f"   A+ insights: {file_results['a_plus_insights']}")

            return file_results

        except Exception as e:
            logger.error(f"‚ùå Error validating {file_path}: {str(e)}")
            self.validation_stats["errors_encountered"] += 1
            return {}

    def generate_comprehensive_report(self, all_results: List[Dict[str, Any]]) -> str:
        """Generate comprehensive quality validation report."""

        # Calculate overall statistics
        if self.validation_stats["insights_tested"] > 0:
            all_scores = []
            total_a_plus = 0
            overall_grade_distribution = {}

            for file_result in all_results:
                if "detailed_results" in file_result:
                    for result in file_result["detailed_results"]:
                        all_scores.append(result["overall_score"])
                        grade = result["grade"]
                        overall_grade_distribution[grade] = (
                            overall_grade_distribution.get(grade, 0) + 1
                        )
                        if result["a_plus_quality"]:
                            total_a_plus += 1

            self.validation_stats["avg_quality_score"] = (
                statistics.mean(all_scores) if all_scores else 0.0
            )
            a_plus_percentage = (total_a_plus / len(all_scores) * 100) if all_scores else 0.0
        else:
            a_plus_percentage = 0.0
            overall_grade_distribution = {}

        # Build grade distribution summary
        grade_summary = ""
        for grade in ["A+", "A", "A-", "B+", "B", "B-", "C+", "C", "F"]:
            count = overall_grade_distribution.get(grade, 0)
            percentage = (
                (count / self.validation_stats["insights_tested"] * 100)
                if self.validation_stats["insights_tested"] > 0
                else 0
            )
            grade_summary += f"   {grade}: {count} insights ({percentage:.1f}%)\n"

        # Build criteria failure summary
        criteria_summary = ""
        for criterion, failures in self.validation_stats["criteria_failures"].items():
            failure_rate = (
                (failures / self.validation_stats["insights_tested"] * 100)
                if self.validation_stats["insights_tested"] > 0
                else 0
            )
            criteria_summary += f"   {criterion}: {failures} failures ({failure_rate:.1f}%)\n"

        report = f"""
üîç COMPREHENSIVE QUALITY VALIDATION REPORT
==========================================

üìä VALIDATION STATISTICS:
   Files Validated: {self.validation_stats['files_validated']}
   Insights Tested: {self.validation_stats['insights_tested']}
   A+ Quality Insights: {total_a_plus} ({a_plus_percentage:.1f}%)
   Average Quality Score: {self.validation_stats['avg_quality_score']:.3f}
   Total Violations Found: {self.validation_stats['violations_found']}
   Errors Encountered: {self.validation_stats['errors_encountered']}

üìà GRADE DISTRIBUTION:
{grade_summary}

‚ùå CRITERIA FAILURE RATES:
{criteria_summary}

‚úÖ A+ QUALITY CRITERIA MET:
   ‚Ä¢ Clarity & Readability: No template artifacts, coherent sentences
   ‚Ä¢ Length Optimization: 15-25 words, concise delivery
   ‚Ä¢ Human Anchoring: 25%+ first-person, action verbs, concrete guidance
   ‚Ä¢ Persona Authenticity: Consistent voice, organic integration
   ‚Ä¢ Spiritual Value: Meaningful wisdom with practical application

üéØ ENHANCEMENT EFFECTIVENESS:
   ‚Ä¢ De-buzzing: Removed spiritual buzzwords and template language
   ‚Ä¢ Length Optimization: Achieved target word counts
   ‚Ä¢ Human Anchoring: Increased first-person perspective and actionable guidance
   ‚Ä¢ Persona Voice: Applied authentic archetypal voices
   ‚Ä¢ Quality Standards: Validated A+ compliance across corpus

üöÄ PRODUCTION READINESS:
   {'‚úÖ READY - Meets A+ standards' if a_plus_percentage >= 80 else '‚ùå NEEDS WORK - Below 80% A+ threshold'}

   Recommendation: {'Deploy to Firebase production' if a_plus_percentage >= 80 else 'Run additional enhancement cycles'}

Generated: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        """

        return report


def main():
    """Main execution function."""
    validator = QualityValidationSuite()

    # Define target directories (same as enhancement scripts)
    project_root = Path("/Users/Maniac_Magee/Documents/XcodeProjects/VybeMVP")

    targets = [
        project_root / "NumerologyData" / "FirebaseNumberMeanings",
        project_root / "NumerologyData" / "FirebasePlanetaryMeanings",
        project_root / "NumerologyData" / "FirebaseZodiacMeanings",
        project_root / "KASPERMLX" / "MLXTraining" / "ContentRefinery" / "Approved",
    ]

    logger.info("üîç Starting COMPREHENSIVE QUALITY VALIDATION...")

    all_results = []

    # Validate each target directory
    for target in targets:
        if target.exists():
            logger.info(f"\nüìÅ Validating quality in: {target}")
            json_files = list(target.glob("*.json"))

            for file_path in json_files:
                file_result = validator.validate_file(file_path)
                if file_result:
                    all_results.append(file_result)
        else:
            logger.warning(f"‚ö†Ô∏è Directory not found: {target}")

    # Generate and save comprehensive report
    report = validator.generate_comprehensive_report(all_results)
    report_path = project_root / "quality_validation_report.md"

    with open(report_path, "w") as f:
        f.write(report)

    # Save detailed results as JSON
    detailed_results_path = project_root / "detailed_validation_results.json"
    with open(detailed_results_path, "w") as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)

    logger.info(f"\nüìã Validation report saved: {report_path}")
    logger.info(f"üìã Detailed results saved: {detailed_results_path}")
    print(report)


if __name__ == "__main__":
    main()
