# ğŸš€ Local LLM Quick Start Guide

## âš¡ **Quick Commands (Copy & Paste)**

### 1. Start Local LLM Server
```bash
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```
**What it does:** Starts Mixtral 46.7B model accessible from iPhone

### 2. Check if Running
```bash
curl http://localhost:11434/api/tags
```
**What it does:** Confirms Ollama is responding

### 3. Stop LLM Server
```bash
killall ollama
```
**What it does:** Stops Ollama completely

## ğŸ“± **App Behavior**

| Device Type | Connects To | Why |
|-------------|-------------|-----|
| **iPhone (Real Device)** | `192.168.1.159:11434` | Your Mac's network IP |
| **iOS Simulator** | `localhost:11434` | Same machine |
| **Production** | `localhost:11434` | Security |

## ğŸ” **Success Indicators**

**In Xcode Console - Look for:**
```
ğŸ”§ Using configured Local LLM endpoint: http://192.168.1.159:11434
âœ… Local LLM Provider ready - Server connection verified
ğŸ‰ Shadow mode successfully activated!
```

**In Terminal - Look for:**
```
[GIN] 2025/08/12 - 20:17:05 | 200 | GET "/api/tags"
time=2025-08-12T19:48:28.017 level=INFO msg="llama runner started"
```

## ğŸ› ï¸ **Troubleshooting**

### "Connection refused" in Xcode logs
1. âœ… Check terminal: Is `OLLAMA_HOST=0.0.0.0:11434 ollama serve` running?
2. âœ… Check network: Are iPhone and Mac on same WiFi?
3. âœ… Kill and restart: `killall ollama` then restart with command above

### Shadow Mode Not Activating
1. âœ… Wait 30 seconds after app launch (model loading takes time)
2. âœ… Check Ollama terminal for model loading messages
3. âœ… Restart app if needed

## ğŸ“‹ **Daily Workflow**

1. **Open Terminal** â†’ Run `OLLAMA_HOST=0.0.0.0:11434 ollama serve`
2. **Wait for** â†’ "Listening on [::]:11434" message
3. **Build & Run** â†’ VybeMVP app on iPhone
4. **Look for** â†’ Shadow mode success logs

## ğŸ”’ **Security Notes**

- âœ… Network IP (192.168.1.159) is LOCAL NETWORK ONLY
- âœ… No internet exposure - only works on your home WiFi
- âœ… Production builds automatically use localhost
- âœ… All inference happens locally on your Mac

## ğŸ’¡ **Pro Tips**

- **Alias Command:** Add to `~/.zshrc`: `alias vybe-llm="OLLAMA_HOST=0.0.0.0:11434 ollama serve"`
- **Terminal Tab:** Keep dedicated terminal tab for Ollama
- **Model Loading:** First startup takes ~20 seconds (46.7B parameters!)
- **Memory Usage:** Uses ~26GB of your Mac's GPU memory

---

*Keep this file - it's safe to reference and contains no secrets!*
